{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess acoustic data and sync to neural data, one session at a time\n",
    "\n",
    "This notebook is a modified version of the *1-preprocess_acoustics* in the chronic ephys processing pipeline\n",
    "\n",
    "If *1-preprocess_acoustics* exits with errors, this notebook allows you to make manual adjustments\n",
    "\n",
    "Common errors include:\n",
    "- data streams cannot be synched (ex. neural and audio data streams are of different lengths)\n",
    "- TTL events were skipped (i.e., the machine clock malfunctioned or SpikeGLX crashed and the data streams terminated at different moments)\n",
    "\n",
    "Use the environment **songproc** to run this notebook\n",
    "\n",
    "(currently using environment spikesort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/lo/envs/songproc/lib/python3.8/site-packages/spikeextractors/__init__.py:21: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if StrictVersion(h5py.__version__) > '2.10.0':\n",
      "2024-08-14 14:51:59,372 root         INFO     Running on pakhi.ucsd.edu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py version > 2.10.0. Some extractors might not work properly. It is recommended to downgrade to version 2.10.0: \n",
      ">>> pip install h5py==2.10.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "import traceback\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/cube/lo/envs/ceciestunepipe')\n",
    "from ceciestunepipe.file import bcistructure as et\n",
    "from ceciestunepipe.util.sound import boutsearch as bs\n",
    "from ceciestunepipe.pipeline import searchbout as sb\n",
    "from ceciestunepipe.util import stimutil as su\n",
    "from ceciestunepipe.util import sglxutil as sglu\n",
    "from ceciestunepipe.util import sglxsync as sy\n",
    "from ceciestunepipe.mods import sglxsync_debug as syd\n",
    "from ceciestunepipe.util.spikeextractors.extractors.spikeglxrecordingextractor import spikeglxrecordingextractor as sglex\n",
    "from ceciestunepipe.util import oeutil as oeu\n",
    "from ceciestunepipe.mods import preproc_sglx, preproc_oe\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.WARNING) # set to logging.INFO if you'd like to see the full readout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default bout detection parameters that work well for zebra finches\n",
    "hparams = {\n",
    "    # spectrogram\n",
    "    'num_freq':1024, # how many channels to use in a spectrogram\n",
    "    'preemphasis':0.97,\n",
    "    'frame_shift_ms':5, # step size for fft\n",
    "    'frame_length_ms':10, # frame length for fft FRAME SAMPLES < NUM_FREQ!!!\n",
    "    'min_level_db':-55, # minimum threshold db for computing spectrogram\n",
    "    'ref_level_db':110, # reference db for computing spectrogram\n",
    "    'sample_rate':None, # sample rate of your data\n",
    "    \n",
    "    # mel filter\n",
    "    'mel_filter':False, # should a mel filter be used?\n",
    "    'num_mels':1024, # how many channels to use in the mel-spectrogram\n",
    "    'fmin':300, # low frequency cutoff for mel filter\n",
    "    'fmax':12000, # high frequency cutoff for mel filter\n",
    "    \n",
    "    # spectrogram inversion\n",
    "    'max_iters':200,\n",
    "    'griffin_lim_iters':20,\n",
    "    'power':1.5,\n",
    "    \n",
    "    # bout searching\n",
    "    'bout_auto_file':'bout_auto.pickle', # extension for saving the auto found files\n",
    "    'bout_sync_file':'bout_sync.pickle', # extension for saving the synchronized auto bouts\n",
    "    'stim_sync_file':'stim_sync.pickle', # extension for saving the synchronized stim if stim session\n",
    "    'bout_curated_file':'bout_curated.pickle', # extension for manually curated files\n",
    "    \n",
    "    # if using deep_bout_search = False, the following parameters will apply for automatic bout detection:\n",
    "    'read_wav_fun':bs.read_npy_chan, # function for loading the wav_like_stream (returns fs, ndarray)\n",
    "    'file_order_fun':bs.sess_file_id, # function for extracting the file ID within the session\n",
    "    'min_segment':20, # minimum length of supra_threshold to consider a 'syllable' (ms)\n",
    "    'min_silence':3000, # minmum distance between groups of syllables to consider separate bouts (ms)\n",
    "    'min_bout':500, # min bout duration (ms)\n",
    "    'peak_thresh_rms':0.55, # threshold (rms) for peak acceptance,\n",
    "    'thresh_rms':0.25, # threshold for detection of syllables\n",
    "    'mean_syl_rms_thresh':0.3, # threshold for acceptance of mean rms across the syllable (relative to rms of the file)\n",
    "    'max_bout':180000, # exclude bouts too long (ms)\n",
    "    'l_p_r_thresh':100, # threshold for n of len_ms/peaks (typycally about 2-3 syllable spans)\n",
    "    'waveform_edges':1000, # get number of ms before and after the edges of the bout for the waveform sample\n",
    "}\n",
    "\n",
    "## other processing parameters\n",
    "n_jobs = 1 # n_jobs for deriving bout info (errors when increased)\n",
    "mic_file_ext = 'npy' # npy method more efficient than wav\n",
    "force_preprocess = False # skip preprocessing for previously failed epochs\n",
    "deep_bout_search = True # detect bouts using deep search -- see ceciestunepipe.mods.bout_detection_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single session params\n",
    "sess_par = {\n",
    "    'bird':'z_r5r13_24',\n",
    "    'sess':'2024-08-07',\n",
    "    'stim_sess':[], # sessions where stimuli were presented\n",
    "    'mic_list':['microphone_M','microphone_F'], # list of mics of interest, by signal name in rig.json\n",
    "    'adc_list':[], # list of adc channels of interest\n",
    "    'stim_list':['wav_stim'], # list of adc chans with the stimulus\n",
    "    'nidq_ttl_list':[], # list of TTL signals form the nidq digital inputs to extract (besides the 'sync')\n",
    "    'ref_stream':'ap_0', # what to synchronize everything to (sglx only, oe already synced)\n",
    "    'trial_tag_chan':2, # sglx, what was the tag channel in the stimulus wave (this should come from meta et. al)\n",
    "    'on_signal':1, # sglx, whether signal on is hi or lo\n",
    "    'sort':'sort_0', # sort index\n",
    "    'ephys_software':'sglx'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess and synchronize recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get experiment structure\n",
    "exp_struct = et.get_exp_struct(sess_par['bird'],sess_par['sess'],sort=sess_par['sort'],ephys_software=sess_par['ephys_software'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing..\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "##### preprocess acoustics #####\n",
    "if sess_par['ephys_software'] == 'sglx':\n",
    "    preproc_sglx.preprocess_session(sess_par,force_redo=True)\n",
    "elif sess_par['ephys_software'] == 'oe':\n",
    "    preproc_oe.preprocess_session(sess_par,force_redo=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbefeb74d6a4f868ff3c340c824cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3364a2d688424369af0f0626da54bbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d14e62ea0f44da598e672679c3ef727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109405de347f4e82931dc542c0b8e00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51355325efab437f8bc378fc6c5ca962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca53b536dd4480943b27ab5ad55c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542d8b55847c4feabc48d46fdc4a2394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8db3c1d45e440ed838e2a820009f7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7613a48d4e4b24b02c710ff92bb936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### derive bout information #####\n",
    "sess_bout_pd = sb.get_all_day_bouts(sess_par,hparams,n_jobs=n_jobs,ephys_software=sess_par['ephys_software'],\n",
    "                                    file_ext=mic_file_ext, deep_search=deep_bout_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process a single epoch at a time, taking care of individual errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store bout info for each epoch\n",
    "bout_syn_pd_all = []\n",
    "# stim sess collect all stim as well\n",
    "if len(sess_par['stim_sess']) > 0:\n",
    "    trial_syn_pd_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0949_g0', '1226_g0', '1227_g0', '1233_g0', '1235_g0', '1244_g0', '1245_g0', '2355_g0', '2631_g0']\n"
     ]
    }
   ],
   "source": [
    "# get epochs\n",
    "sess_epochs = et.list_ephys_epochs(sess_par)\n",
    "print(sess_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging: check that all streams are the same length (address ttl events error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nidq recording ends at 7:21\n",
      "ap_0 recording ends at 7:21\n",
      "wav recording ends at 7:21\n"
     ]
    }
   ],
   "source": [
    "sess_par['epoch'] = '1245_g0' # problematic epoch\n",
    "epoch_struct = et.sgl_struct(sess_par,sess_par['epoch'],ephys_software=sess_par['ephys_software'])\n",
    "\n",
    "# get epoch files\n",
    "sgl_folders, sgl_files = sglu.sgl_file_struct(epoch_struct['folders']['sglx'])\n",
    "run_meta_files = {k:v[0] for k,v in sgl_files.items()}\n",
    "run_recordings = {k:sglex.SpikeGLXRecordingExtractor(sglu.get_data_meta_path(v)[0]) for k,v in run_meta_files.items()}\n",
    "\n",
    "# get streams, from raw recording extractors and preprocessed data\n",
    "all_streams = list(run_recordings.keys()) + ['wav'] ### might want to just remove this\n",
    "# get sync pattern\n",
    "all_syn_dict = {k:sy.get_syn_pattern(run_recordings,epoch_struct,k,force=False) for k in all_streams}\n",
    "\n",
    "for stream in all_syn_dict.keys():\n",
    "    time_end = np.shape(all_syn_dict[stream]['t_0'])[0]/all_syn_dict[stream]['s_f']/60\n",
    "    print(stream+' recording ends at '+str(int(np.floor(time_end)))+':'+f\"{round((time_end % 1)*60):02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for skipped heartbeats in nidq stream:\n",
      "Event array has 883 events\n",
      "No skipped heartbeats\n",
      "\n",
      "Checking for skipped heartbeats in ap_0 stream:\n",
      "Event array has 883 events\n",
      "No skipped heartbeats\n",
      "\n",
      "Checking for skipped heartbeats in wav stream:\n",
      "Event array has 883 events\n",
      "No skipped heartbeats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## debugging: check if streams have skipped heartbeats\n",
    "def check_skipped(one_syn_dict: dict, round_ms=10):\n",
    "    no_skips = True\n",
    "    evt_arr = one_syn_dict['evt_arr']\n",
    "    evt_t = one_syn_dict['t_0'][evt_arr[0]]\n",
    "    print('Event array has {} events'.format(evt_arr.size//2))\n",
    "    \n",
    "    # get the unique periods, rounded at round_ms (default 50 ms)\n",
    "    evt_period_ms = np.unique(np.round((np.unique(np.diff(evt_t))*1000)/round_ms)*round_ms).astype(int)\n",
    "    if evt_period_ms.size > 1:\n",
    "        evt_diff = (np.round(np.diff(evt_t)*1000/round_ms)*round_ms).astype(int)\n",
    "        no_skips = False\n",
    "        bad_periods = np.where(evt_diff != np.argmax(np.bincount(evt_diff)))[0]\n",
    "        print('More than 1 different periods detected: {}'.format(evt_period_ms))\n",
    "        print('Most periods equal to {} -- bad periods:'.format(np.argmax(np.bincount(evt_diff))))\n",
    "        for bp in bad_periods:\n",
    "            print('evt_diff['+str(bp)+']='+str(evt_diff[bp]))\n",
    "    \n",
    "    # check that the diff between every other edge is zero\n",
    "    period_diff = np.hstack([np.diff(evt_arr[1][1:][::2]), np.diff(evt_arr[1][::2])])\n",
    "    if not (all(period_diff==0)):\n",
    "        no_skips = False\n",
    "        print('Difference between corresponding periodic edges is not zero: {}'.format(np.unique(period_diff)))\n",
    "    \n",
    "    if no_skips: print('No skipped heartbeats')\n",
    "\n",
    "for stream in all_syn_dict.keys():\n",
    "    print('Checking for skipped heartbeats in',stream,'stream:')\n",
    "    check_skipped(all_syn_dict[stream]);print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record epochs with mismatched streams and which stream is shortest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_streams = {\n",
    "    '0949_g0': (True, 'nidq'),\n",
    "    '1226_g0': (True, 'nidq'),\n",
    "    '1227_g0': (True, 'nidq'),\n",
    "    '1233_g0': (True, 'nidq'),\n",
    "    '1235_g0': (True, 'nidq'),\n",
    "    '1244_g0': (True, 'nidq'),\n",
    "    '1245_g0': (True, 'nidq'),\n",
    "    '2355_g0': (False,),\n",
    "    '2631_g0': (False,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_r5r13_24 2024-08-07 0949_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 1226_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 1227_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 1233_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 1235_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 1245_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 2355_g0 syncing..\n",
      "z_r5r13_24 2024-08-07 2631_g0 syncing..\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# loop through epochs:\n",
    "epoch_list = sess_epochs # process all epochs\n",
    "\n",
    "for this_epoch in epoch_list:\n",
    "    \n",
    "    sess_par['epoch'] = this_epoch\n",
    "    epoch_struct = et.sgl_struct(sess_par,sess_par['epoch'],ephys_software=sess_par['ephys_software'])\n",
    "\n",
    "    ##### synchronization - sglx #####\n",
    "    print(sess_par['bird'],sess_par['sess'],this_epoch,'syncing..')\n",
    "    if sess_par['ephys_software'] == 'sglx':\n",
    "        # get epoch files\n",
    "        sgl_folders, sgl_files = sglu.sgl_file_struct(epoch_struct['folders']['sglx'])\n",
    "        run_meta_files = {k:v[0] for k,v in sgl_files.items()}\n",
    "        run_recordings = {k:sglex.SpikeGLXRecordingExtractor(sglu.get_data_meta_path(v)[0]) for k,v in run_meta_files.items()}\n",
    "\n",
    "        # get streams, from raw recording extractors and preprocessed data\n",
    "        all_streams = list(run_recordings.keys()) + ['wav'] ### might want to just remove this\n",
    "        # get sync pattern\n",
    "        all_syn_dict = {k:sy.get_syn_pattern(run_recordings,epoch_struct,k,force=False) for k in all_streams}\n",
    "        # run sync\n",
    "        if mismatched_streams[this_epoch][0]:\n",
    "            syd.sync_all_mismatched_streams(all_syn_dict,sess_par['ref_stream'],mismatched_streams[this_epoch][1],force=False)\n",
    "        else:\n",
    "            sy.sync_all(all_syn_dict,sess_par['ref_stream'],force=False)\n",
    "\n",
    "        # load bouts\n",
    "        hparams, bout_pd = sb.load_bouts(sess_par['bird'],sess_par['sess'],'', derived_folder='bouts_sglx',bout_file_key='bout_auto_file')\n",
    "        # keep only epoch bouts\n",
    "        logger.info('bouts from this epoch {}'.format(sess_par['epoch']))\n",
    "        drop_condition = ~bout_pd['file'].str.contains(sess_par['epoch'])\n",
    "        bout_pd.drop(bout_pd[drop_condition].index, inplace=True)\n",
    "        bout_pd.reset_index(drop=True, inplace=True)\n",
    "        # sync bouts to spike time base\n",
    "        if mismatched_streams[this_epoch][0]:\n",
    "            bout_dict, bout_syn_pd = syd.bout_dict_from_pd_mismatched_streams(bout_pd,all_syn_dict,s_f_key='wav')\n",
    "        else:\n",
    "            bout_dict, bout_syn_pd = sy.bout_dict_from_pd(bout_pd,all_syn_dict,s_f_key='wav')\n",
    "        # store epoch synced bout info\n",
    "        bout_syn_pd['bird'] = sess_par['bird']\n",
    "        bout_syn_pd['sess'] = sess_par['sess']\n",
    "        bout_syn_pd['epoch'] = sess_par['epoch']\n",
    "        bout_syn_pd_all.append(bout_syn_pd)\n",
    "        # save synced bouts\n",
    "        bout_dict_path = os.path.join(epoch_struct['folders']['derived'],'bout_dict_ap0.pkl')\n",
    "        with open(bout_dict_path, 'wb') as handle:\n",
    "            pickle.dump(bout_dict, handle)\n",
    "        bout_pd_path = os.path.join(epoch_struct['folders']['derived'],'bout_pd_ap0.pkl')\n",
    "        bout_pd.to_pickle(bout_pd_path)\n",
    "        logger.info('saved syncronized bout dict and pandas dataframe to {}, {}'.format(bout_dict_path, bout_pd_path))\n",
    "\n",
    "        if len(sess_par['stim_sess']) > 0:\n",
    "            # syn_ttl comes from the digital pin, syn_sine_ttl from the sine\n",
    "            event_name = 'wav_stim'\n",
    "            ttl_ev_name = event_name + '_sync_sine_ttl' \n",
    "            # get the events npy file\n",
    "            npy_stim_path = os.path.join(epoch_struct['folders']['derived'],ttl_ev_name + '_evt.npy')\n",
    "            stream_stim_path = os.path.join(epoch_struct['folders']['derived'],event_name + '.npy')\n",
    "            trial_ttl = np.load(npy_stim_path)\n",
    "            # epoch may not have trials - if so ttl file will be empty\n",
    "            if len(trial_ttl) > 0:\n",
    "                trial_stream = np.load(stream_stim_path,mmap_mode='r')\n",
    "                # get sampling frequency\n",
    "                stim_s_f = int(all_syn_dict['nidq']['s_f'])\n",
    "                # load the stimulus name - frequency tag dictionary\n",
    "                stim_tags_dict = preproc_sglx.load_stim_tags_dict(sess_par['stim_sess'],sess_par['bird'])\n",
    "                # get trial tagged dataframe\n",
    "                trial_tagged_pd = su.get_trials_pd(trial_ttl, trial_stream, stim_s_f,on_signal=sess_par['on_signal'],\n",
    "                                                   tag_chan=sess_par['trial_tag_chan'],stim_tags_dict=stim_tags_dict,\n",
    "                                                   trial_is_onof=True)\n",
    "                # sync stim\n",
    "                trial_dict, trial_syn_pd = sy.trial_syn_from_pd(trial_tagged_pd,all_syn_dict,s_f_key='nidq')\n",
    "                # store epoch synced stim info\n",
    "                trial_syn_pd['bird'] = sess_par['bird']\n",
    "                trial_syn_pd['sess'] = sess_par['sess']\n",
    "                trial_syn_pd['epoch'] = this_epoch\n",
    "                trial_syn_pd_all.append(trial_syn_pd)\n",
    "                # save synced stim\n",
    "                stim_dict_path = os.path.join(epoch_struct['folders']['derived'],'stim_dict_ap0.pkl')\n",
    "                stim_pd_path = os.path.join(epoch_struct['folders']['derived'],'stim_pd_ap0.pkl')\n",
    "                with open(stim_dict_path,'wb') as handle:\n",
    "                    pickle.dump(trial_dict,handle)\n",
    "                trial_syn_pd.to_pickle(stim_pd_path)\n",
    "                logger.info('saved syncronized stim dict and pandas dataframe to {}, {}'.format(stim_dict_path, stim_pd_path))\n",
    "\n",
    "    ###### synchronization - oe #####\n",
    "    elif sess_par['ephys_software'] == 'oe':\n",
    "        # get epoch files\n",
    "        run_recordings = {'oeb':preproc_oe.get_oe_cont_recording(exp_struct,this_epoch)}\n",
    "\n",
    "        # make an all_syn_dict\n",
    "        mic_file_name = os.path.join(exp_struct['folders']['derived'],this_epoch,'wav_mic-npy_meta.pickle')\n",
    "        with open(mic_file_name, 'rb') as handle:\n",
    "            wav_mic_meta = pickle.load(handle)\n",
    "        all_syn_dict = {'wav': {'s_f':wav_mic_meta['s_f']}, \n",
    "                       'ap_0': {'s_f':run_recordings['oeb'].get_sampling_frequency()},\n",
    "                       'nidq': {'s_f':run_recordings['oeb'].get_sampling_frequency()}}\n",
    "        # make bouts pandas file for this session - match sglx format, streams already synced\n",
    "        bout_oe_struct = et.get_exp_struct(sess_par['bird'],sess_par['sess'],sort=sess_par['sort'],ephys_software='bouts_oe')\n",
    "        bout_pd_path = os.path.join(bout_oe_struct['folders']['derived'], 'bout_auto.pickle')\n",
    "        bout_syn_pd = pd.read_pickle(bout_pd_path)\n",
    "        bout_dict = preproc_oe.bout_dict_from_pd(bout_syn_pd,all_syn_dict)\n",
    "        # store epoch synced bout info\n",
    "        bout_syn_pd['bird'] = sess_par['bird']\n",
    "        bout_syn_pd['sess'] = sess_par['sess']\n",
    "        bout_syn_pd['epoch'] = this_epoch\n",
    "        bout_syn_pd_all.append(bout_syn_pd)\n",
    "        # save synced bouts\n",
    "        bout_dict_path = os.path.join(epoch_struct['folders']['derived'],'bout_dict_oe.pkl')\n",
    "        bout_pd_path = os.path.join(epoch_struct['folders']['derived'],'bout_pd_oe.pkl')\n",
    "        with open(bout_dict_path,'wb') as handle:\n",
    "            pickle.dump(bout_dict,handle)\n",
    "        bout_syn_pd.to_pickle(bout_pd_path)\n",
    "\n",
    "        if len(sess_par['stim_sess']) > 0:\n",
    "            # this epoch name - get recording events path\n",
    "            raw_folder = exp_struct['folders']['oe']\n",
    "            epoch_path = os.path.join(raw_folder,this_epoch)\n",
    "            node_path = preproc_oe.get_default_node(exp_struct,this_epoch)\n",
    "            rec_path = preproc_oe.get_default_recording(node_path)\n",
    "            events_path = os.path.join(rec_path,'events/Network_Events-102.0/TEXT_group_1/')\n",
    "            # load stim lables / onsets\n",
    "            stim_labels = np.load(os.path.join(events_path,'text.npy'))\n",
    "            stim_onsets = np.load(os.path.join(events_path,'timestamps.npy'))\n",
    "\n",
    "            # get stim onsets and offsets\n",
    "            stim_on_all = []; stim_off_all = []; \n",
    "            stim_proc_path_all = []; stim_exp_path_all = [];\n",
    "            stim_map_dir_all = []; stim_id_all = [];\n",
    "            # loop through stim\n",
    "            for stim_i in range(len(stim_labels)):\n",
    "                this_stim_label = stim_labels[stim_i].astype('str')\n",
    "                this_stim_onset = stim_onsets[stim_i]\n",
    "                if this_stim_label[:4] == 'stim':\n",
    "                    stim_exp_file = this_stim_label[5:]\n",
    "                    # get stim preprocessing directory\n",
    "                    stim_file_split = stim_exp_file.split('/')\n",
    "                    stim_map_i = np.where([stim_file_split[i] in list(stim_map_dict.keys()) for i in range(len(stim_file_split))])[0][0]\n",
    "                    stim_map_dir = stim_map_dict[stim_file_split[stim_map_i]]\n",
    "                    # get remaining stim file path - identical for experiment and preprocessing\n",
    "                    remaining_stim_file = '/'.join(stim_file_split[stim_map_i+1:])\n",
    "                    # processing file location\n",
    "                    stim_file = os.path.join(stim_map_dir,remaining_stim_file)\n",
    "                    # load stim and get length\n",
    "                    sf,this_wav = wavfile.read(stim_file,mmap=True)\n",
    "                    stim_len = this_wav.shape[0]/sf\n",
    "                    # get length of stim in samples - round up\n",
    "                    stim_samp_len = int(np.ceil(stim_len * bout_dict['s_f']))\n",
    "                    # get stim on / off\n",
    "                    stim_on_all.append(this_stim_onset)\n",
    "                    stim_off_all.append(this_stim_onset+stim_samp_len)  \n",
    "                    stim_proc_path_all.append(stim_file)\n",
    "                    stim_exp_path_all.append(stim_exp_file)\n",
    "                    stim_map_dir_all.append(stim_map_dir)\n",
    "                    stim_id_all.append(remaining_stim_file)\n",
    "\n",
    "            # make into a pd - oe already synced\n",
    "            stim_on_all_np = np.array(stim_on_all).astype('int')\n",
    "            stim_off_all_np = np.array(stim_off_all).astype('int')\n",
    "            stim_on_all_np_ms = 1000*(stim_on_all_np/bout_dict['s_f'])\n",
    "            stim_off_all_np_ms = 1000*(stim_off_all_np/bout_dict['s_f'])\n",
    "            trial_syn_pd = pd.DataFrame(np.vstack([stim_on_all_np,\n",
    "                                                stim_off_all_np,\n",
    "                                                stim_on_all_np_ms,\n",
    "                                                stim_off_all_np_ms,\n",
    "                                                stim_off_all_np_ms-stim_on_all_np_ms,\n",
    "                                                stim_proc_path_all,\n",
    "                                                stim_exp_path_all,\n",
    "                                                stim_map_dir_all,\n",
    "                                                stim_id_all]).T,\n",
    "            columns=['start_sample','end_sample','start_ms','end_ms','len_ms',\n",
    "                     'proc_file','exp_file','map_dir','stim_id'])\n",
    "            trial_syn_pd['start_sample'] = trial_syn_pd['start_sample'].astype('int')\n",
    "            trial_syn_pd['end_sample'] = trial_syn_pd['end_sample'].astype('int')\n",
    "            trial_syn_pd['start_ms'] = trial_syn_pd['start_ms'].astype('float')\n",
    "            trial_syn_pd['len_ms'] = trial_syn_pd['len_ms'].astype('float')\n",
    "            # store epoch synced stim info\n",
    "            trial_syn_pd['bird'] = sess_par['bird']\n",
    "            trial_syn_pd['sess'] = sess_par['sess']\n",
    "            trial_syn_pd['epoch'] = this_epoch\n",
    "            trial_syn_pd_all.append(trial_syn_pd)\n",
    "            trial_dict = {\n",
    "                's_f': all_syn_dict['wav']['s_f'],\n",
    "                'ap_0':all_syn_dict['ap_0']['s_f'],\n",
    "                'nidq':all_syn_dict['nidq']['s_f'],\n",
    "                'start_ms':trial_syn_pd['start_ms'],\n",
    "                'len_ms':trial_syn_pd['len_ms'],\n",
    "                'start_sample':trial_syn_pd['start_sample'],\n",
    "                'end_sample':trial_syn_pd['end_sample'],\n",
    "                'proc_file':trial_syn_pd['proc_file'],\n",
    "                'exp_file':trial_syn_pd['exp_file'],\n",
    "                'map_dir':trial_syn_pd['map_dir'],\n",
    "                'stim_id':trial_syn_pd['stim_id']}\n",
    "            # save synced stim\n",
    "            stim_dict_path = os.path.join(epoch_struct['folders']['derived'],'stim_dict_ap0.pkl')\n",
    "            stim_pd_path = os.path.join(epoch_struct['folders']['derived'],'stim_pd_ap0.pkl')\n",
    "            with open(stim_dict_path,'wb') as handle:\n",
    "                pickle.dump(trial_dict,handle)\n",
    "            trial_syn_pd.to_pickle(stim_pd_path)\n",
    "            logger.info('saved syncronized stim dict and pandas dataframe to {}, {}'.format(stim_dict_path, stim_pd_path))\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After handling all errors, save outputs and log preprocessing complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate list of synced bout data frames from each epoch and save\n",
    "bout_syn_pd_all_cat = pd.concat(bout_syn_pd_all)\n",
    "sb.save_auto_bouts(bout_syn_pd_all_cat,sess_par,hparams,software=sess_par['ephys_software'],bout_file_key='bout_sync_file')\n",
    "\n",
    "# stim sess save the all sync epoch stim data frame as well\n",
    "if len(sess_par['stim_sess']) > 0:\n",
    "    trial_syn_pd_all_cat = pd.concat(trial_syn_pd_all)\n",
    "    sb.save_auto_bouts(trial_syn_pd_all_cat,sess_par,hparams,software=sess_par['ephys_software'],bout_file_key='stim_sync_file')\n",
    "\n",
    "# # log preprocessing complete without error\n",
    "# log_dir = os.path.join('/mnt/cube/chronic_ephys/log', sess_par['bird'], sess_par['sess'])\n",
    "# with open(os.path.join(log_dir,'preprocessing.log'), 'w') as f:\n",
    "#     f.write(sess_par['bird']+' '+sess_par['sess']+' preprocessing complete without error\\nEpochs '+', '.join(sess_epochs)+' processed\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To look up lengths of recordings to stitch them together in 2-curate_acoustics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples: 14681207\n"
     ]
    }
   ],
   "source": [
    "sess_par['epoch'] = '1235_g0'\n",
    "epoch_struct = et.sgl_struct(sess_par,sess_par['epoch'],ephys_software=sess_par['ephys_software'])\n",
    "all_syn_dict = {k:sy.get_syn_pattern(run_recordings,epoch_struct,k,force=False) for k in all_streams}\n",
    "print('n samples:',np.shape(all_syn_dict['ap_0']['t_0'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate: 29999.844262295082\n"
     ]
    }
   ],
   "source": [
    "print('sampling rate:',all_syn_dict['ap_0']['s_f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "songproc",
   "language": "python",
   "name": "songproc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
