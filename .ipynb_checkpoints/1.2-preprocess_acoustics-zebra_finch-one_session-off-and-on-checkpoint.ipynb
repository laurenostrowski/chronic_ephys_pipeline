{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess acoustic data and sync to neural data, one session at a time\n",
    "\n",
    "This notebook is a modified version of the *1-preprocess_acoustics* in the chronic ephys processing pipeline\n",
    "\n",
    "If *1-preprocess_acoustics* exits with errors, this notebook allows you to make manual adjustments\n",
    "\n",
    "Common errors include:\n",
    "- data streams cannot be synched (ex. neural and audio data streams are of different lengths)\n",
    "- TTL events were skipped (i.e., the machine clock malfunctioned or SpikeGLX crashed and the data streams terminated at different moments)\n",
    "\n",
    "Use the environment **songproc** to run this notebook\n",
    "\n",
    "(currently using environment spikesort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/lo/envs/songproc/lib/python3.8/site-packages/spikeextractors/__init__.py:21: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if StrictVersion(h5py.__version__) > '2.10.0':\n",
      "2024-08-14 12:19:41,818 root         INFO     Running on pakhi.ucsd.edu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py version > 2.10.0. Some extractors might not work properly. It is recommended to downgrade to version 2.10.0: \n",
      ">>> pip install h5py==2.10.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "import traceback\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/cube/lo/envs/ceciestunepipe')\n",
    "from ceciestunepipe.file import bcistructure as et\n",
    "from ceciestunepipe.util.sound import boutsearch as bs\n",
    "from ceciestunepipe.pipeline import searchbout as sb\n",
    "from ceciestunepipe.util import stimutil as su\n",
    "from ceciestunepipe.util import sglxutil as sglu\n",
    "from ceciestunepipe.util import sglxsync as sy\n",
    "from ceciestunepipe.mods import sglxsync_debug as syd\n",
    "from ceciestunepipe.util.spikeextractors.extractors.spikeglxrecordingextractor import spikeglxrecordingextractor as sglex\n",
    "from ceciestunepipe.util import oeutil as oeu\n",
    "from ceciestunepipe.mods import preproc_sglx, preproc_oe\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.WARNING) # set to logging.INFO if you'd like to see the full readout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default bout detection parameters that work well for zebra finches\n",
    "hparams = {\n",
    "    # spectrogram\n",
    "    'num_freq':1024, # how many channels to use in a spectrogram\n",
    "    'preemphasis':0.97,\n",
    "    'frame_shift_ms':5, # step size for fft\n",
    "    'frame_length_ms':10, # frame length for fft FRAME SAMPLES < NUM_FREQ!!!\n",
    "    'min_level_db':-55, # minimum threshold db for computing spectrogram\n",
    "    'ref_level_db':110, # reference db for computing spectrogram\n",
    "    'sample_rate':None, # sample rate of your data\n",
    "    \n",
    "    # mel filter\n",
    "    'mel_filter':False, # should a mel filter be used?\n",
    "    'num_mels':1024, # how many channels to use in the mel-spectrogram\n",
    "    'fmin':300, # low frequency cutoff for mel filter\n",
    "    'fmax':12000, # high frequency cutoff for mel filter\n",
    "    \n",
    "    # spectrogram inversion\n",
    "    'max_iters':200,\n",
    "    'griffin_lim_iters':20,\n",
    "    'power':1.5,\n",
    "    \n",
    "    # bout searching\n",
    "    'bout_auto_file':'bout_auto.pickle', # extension for saving the auto found files\n",
    "    'bout_sync_file':'bout_sync.pickle', # extension for saving the synchronized auto bouts\n",
    "    'stim_sync_file':'stim_sync.pickle', # extension for saving the synchronized stim if stim session\n",
    "    'bout_curated_file':'bout_curated.pickle', # extension for manually curated files\n",
    "    \n",
    "    # if using deep_bout_search = False, the following parameters will apply for automatic bout detection:\n",
    "    'read_wav_fun':bs.read_npy_chan, # function for loading the wav_like_stream (returns fs, ndarray)\n",
    "    'file_order_fun':bs.sess_file_id, # function for extracting the file ID within the session\n",
    "    'min_segment':20, # minimum length of supra_threshold to consider a 'syllable' (ms)\n",
    "    'min_silence':3000, # minmum distance between groups of syllables to consider separate bouts (ms)\n",
    "    'min_bout':500, # min bout duration (ms)\n",
    "    'peak_thresh_rms':0.55, # threshold (rms) for peak acceptance,\n",
    "    'thresh_rms':0.25, # threshold for detection of syllables\n",
    "    'mean_syl_rms_thresh':0.3, # threshold for acceptance of mean rms across the syllable (relative to rms of the file)\n",
    "    'max_bout':180000, # exclude bouts too long (ms)\n",
    "    'l_p_r_thresh':100, # threshold for n of len_ms/peaks (typycally about 2-3 syllable spans)\n",
    "    'waveform_edges':1000, # get number of ms before and after the edges of the bout for the waveform sample\n",
    "}\n",
    "\n",
    "## other processing parameters\n",
    "n_jobs = 1 # n_jobs for deriving bout info (errors when increased)\n",
    "mic_file_ext = 'npy' # npy method more efficient than wav\n",
    "force_preprocess = False # skip preprocessing for previously failed epochs\n",
    "deep_bout_search = True # detect bouts using deep search -- see ceciestunepipe.mods.bout_detection_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single session params\n",
    "sess_par = {\n",
    "    'bird':'z_r5r13_24',\n",
    "    'sess':'2024-08-08',\n",
    "    'stim_sess':[], # sessions where stimuli were presented\n",
    "    'mic_list':['microphone_M','microphone_F'], # list of mics of interest, by signal name in rig.json\n",
    "    'adc_list':[], # list of adc channels of interest\n",
    "    'stim_list':['wav_stim'], # list of adc chans with the stimulus\n",
    "    'nidq_ttl_list':[], # list of TTL signals form the nidq digital inputs to extract (besides the 'sync')\n",
    "    'ref_stream':'ap_0', # what to synchronize everything to (sglx only, oe already synced)\n",
    "    'trial_tag_chan':2, # sglx, what was the tag channel in the stimulus wave (this should come from meta et. al)\n",
    "    'on_signal':1, # sglx, whether signal on is hi or lo\n",
    "    'sort':'sort_0', # sort index\n",
    "    'ephys_software':'sglx'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess and synchronize recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get experiment structure\n",
    "exp_struct = et.get_exp_struct(sess_par['bird'],sess_par['sess'],sort=sess_par['sort'],ephys_software=sess_par['ephys_software'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing..\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "##### preprocess acoustics #####\n",
    "if sess_par['ephys_software'] == 'sglx':\n",
    "    preproc_sglx.preprocess_session(sess_par,force_redo=True)\n",
    "elif sess_par['ephys_software'] == 'oe':\n",
    "    preproc_oe.preprocess_session(sess_par,force_redo=True)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042c7905a373496688d27d7ffc3bdc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d860a23332491c94451c5fac66d445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1103bd78dd4ac99f761da8e8c3d0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572abb727afe463bae2a871b81b2b389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### derive bout information #####\n",
    "sess_bout_pd = sb.get_all_day_bouts(sess_par,hparams,n_jobs=n_jobs,ephys_software=sess_par['ephys_software'],\n",
    "                                    file_ext=mic_file_ext, deep_search=deep_bout_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process a single epoch at a time, taking care of individual errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store bout info for each epoch\n",
    "bout_syn_pd_all = []\n",
    "# stim sess collect all stim as well\n",
    "if len(sess_par['stim_sess']) > 0:\n",
    "    trial_syn_pd_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0700_g0', '1000_g0', '2355_g0', '2705_g0']\n"
     ]
    }
   ],
   "source": [
    "# get epochs\n",
    "sess_epochs = et.list_ephys_epochs(sess_par)\n",
    "print(sess_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging: check that all streams are the same length (address ttl events error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nidq recording ends at 175:20\n",
      "ap_0 recording ends at 175:17\n",
      "wav recording ends at 175:19\n"
     ]
    }
   ],
   "source": [
    "sess_par['epoch'] = '1000_g0' # problematic epoch\n",
    "epoch_struct = et.sgl_struct(sess_par,sess_par['epoch'],ephys_software=sess_par['ephys_software'])\n",
    "\n",
    "# get epoch files\n",
    "sgl_folders, sgl_files = sglu.sgl_file_struct(epoch_struct['folders']['sglx'])\n",
    "run_meta_files = {k:v[0] for k,v in sgl_files.items()}\n",
    "run_recordings = {k:sglex.SpikeGLXRecordingExtractor(sglu.get_data_meta_path(v)[0]) for k,v in run_meta_files.items()}\n",
    "\n",
    "# get streams, from raw recording extractors and preprocessed data\n",
    "all_streams = list(run_recordings.keys()) + ['wav'] ### might want to just remove this\n",
    "# get sync pattern\n",
    "all_syn_dict = {k:sy.get_syn_pattern(run_recordings,epoch_struct,k,force=False) for k in all_streams}\n",
    "\n",
    "for stream in all_syn_dict.keys():\n",
    "    time_end = np.shape(all_syn_dict[stream]['t_0'])[0]/all_syn_dict[stream]['s_f']/60\n",
    "    print(stream+' recording ends at '+str(int(np.floor(time_end)))+':'+f\"{round((time_end % 1)*60):02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for skipped heartbeats in nidq stream:\n",
      "Event array has 21040 events\n",
      "More than 1 different periods detected: [130 440 500]\n",
      "Most periods equal to 500 -- bad periods:\n",
      "evt_diff[13797]=440\n",
      "evt_diff[13798]=130\n",
      "\n",
      "Checking for skipped heartbeats in ap_0 stream:\n",
      "Event array has 21034 events\n",
      "More than 1 different periods detected: [140 500]\n",
      "Most periods equal to 500 -- bad periods:\n",
      "evt_diff[13792]=140\n",
      "\n",
      "Checking for skipped heartbeats in wav stream:\n",
      "Event array has 21040 events\n",
      "More than 1 different periods detected: [130 440 500]\n",
      "Most periods equal to 500 -- bad periods:\n",
      "evt_diff[13797]=440\n",
      "evt_diff[13798]=130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## debugging: check if streams have skipped heartbeats\n",
    "def check_skipped(one_syn_dict: dict, round_ms=10):\n",
    "    no_skips = True\n",
    "    evt_arr = one_syn_dict['evt_arr']\n",
    "    evt_t = one_syn_dict['t_0'][evt_arr[0]]\n",
    "    print('Event array has {} events'.format(evt_arr.size//2))\n",
    "    \n",
    "    # get the unique periods, rounded at round_ms (default 50 ms)\n",
    "    evt_period_ms = np.unique(np.round((np.unique(np.diff(evt_t))*1000)/round_ms)*round_ms).astype(int)\n",
    "    if evt_period_ms.size > 1:\n",
    "        evt_diff = (np.round(np.diff(evt_t)*1000/round_ms)*round_ms).astype(int)\n",
    "        no_skips = False\n",
    "        bad_periods = np.where(evt_diff != np.argmax(np.bincount(evt_diff)))[0]\n",
    "        print('More than 1 different periods detected: {}'.format(evt_period_ms))\n",
    "        print('Most periods equal to {} -- bad periods:'.format(np.argmax(np.bincount(evt_diff))))\n",
    "        for bp in bad_periods:\n",
    "            print('evt_diff['+str(bp)+']='+str(evt_diff[bp]))\n",
    "    \n",
    "    # check that the diff between every other edge is zero\n",
    "    period_diff = np.hstack([np.diff(evt_arr[1][1:][::2]), np.diff(evt_arr[1][::2])])\n",
    "    if not (all(period_diff==0)):\n",
    "        no_skips = False\n",
    "        print('Difference between corresponding periodic edges is not zero: {}'.format(np.unique(period_diff)))\n",
    "    \n",
    "    if no_skips: print('No skipped heartbeats')\n",
    "\n",
    "for stream in all_syn_dict.keys():\n",
    "    print('Checking for skipped heartbeats in',stream,'stream:')\n",
    "    check_skipped(all_syn_dict[stream]);print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to\n",
    "- cut all streams at evt_t = 13971\n",
    "- resume streams at evt_t = [total length of stream] - 7240\n",
    "    - 13794 for ap_0\n",
    "    - 13800 for nidq\n",
    "    - 13800 for wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ceciestunepipe.util import syncutil as scu\n",
    "\n",
    "def sync_all_mismatched_streams(all_syn_dict: dict, ref_stream: str, short_stream: str, force=False) -> dict:\n",
    "    ref_syn_dict = all_syn_dict[ref_stream]\n",
    "\n",
    "    first_len_evt_arr = 13971 # number of events captured before streams were cut off\n",
    "    first_len = first_len_evt_arr/2 # time when streams got cut off (s)\n",
    "\n",
    "    last_len_evt_arr = 7240 # number of events captured after streams resumed\n",
    "    last_len = last_len_evt_arr/2 # time after streams were resumed (s)\n",
    "    \n",
    "    # new_len = np.shape(all_syn_dict[short_stream]['t_0'])[0]/all_syn_dict[short_stream]['s_f'] # length of short stream (s)\n",
    "    ref_end = int(first_len * all_syn_dict[ref_stream]['s_f']) # where to truncate ref_stream (samples)\n",
    "    ref_restart = int((np.shape(all_syn_dict[ref_stream]['t_0'])[0] - last_len) * all_syn_dict[ref_stream]['s_f']) # where to restart ref_stream (samples)\n",
    "    ref_len_evt_arr = np.shape(all_syn_dict[short_stream]['evt_arr'])[1] # number of events captured in ref stream\n",
    "    \n",
    "    for one_stream, one_syn_dict in all_syn_dict.items():\n",
    "        if one_stream==ref_stream:\n",
    "            continue\n",
    "            \n",
    "        print(' sync {}...'.format(one_stream))\n",
    "        \n",
    "        t_0_folder = os.path.split(one_syn_dict['t_0_path'])[0]\n",
    "        t_p_path = os.path.join(t_0_folder, '{}-tp.npy'.format(one_stream))\n",
    "        \n",
    "        if not(os.path.exists(t_p_path) and (force is False)):\n",
    "            print('  t_prime file {} not found or forced computation, getting the events'.format(t_p_path))\n",
    "            \n",
    "            # Edit one_stream to length of short_stream:\n",
    "            one_end = int(first_len * all_syn_dict[one_stream]['s_f']) # where to truncate one_stream (samples)\n",
    "            one_restart = int((np.shape(all_syn_dict[one_stream]['t_0'])[0] - last_len) * all_syn_dict[one_stream]['s_f']) # where to resume one_stream (samples)\n",
    "            \n",
    "            # Sync to ref stream\n",
    "            one_len_evt_arr = np.shape(one_syn_dict['evt_arr'])[1]\n",
    "            t_prime = scu.sync_to_pattern(np.concatenate((one_syn_dict['evt_arr'][:,:first_len_evt_arr],\n",
    "                                                         one_syn_dict['evt_arr'][:,one_len_evt_arr-last_len_evt_arr:]), axis=1), \n",
    "                                         np.concatenate((one_syn_dict['t_0'][:one_end],\n",
    "                                                         one_syn_dict['t_0'][one_restart:])),\n",
    "                                         np.concatenate((ref_syn_dict['evt_arr'][:,:first_len_evt_arr],\n",
    "                                                         ref_syn_dict['evt_arr'][:,ref_len_evt_arr-last_len_evt_arr:]), axis=1),\n",
    "                                         np.concatenate((ref_syn_dict['t_0'][:ref_end],\n",
    "                                                         ref_syn_dict['t_0'][ref_restart:]))\n",
    "                                        )\n",
    "            print('    saving t_prime array to ' + t_p_path)\n",
    "            np.save(t_p_path, t_prime)\n",
    "        \n",
    "            # clear the memory, then load as memmap\n",
    "            del t_prime\n",
    "        \n",
    "            one_syn_dict['t_p_path'] = t_p_path\n",
    "            # save the dict with the path to the sync it\n",
    "            print('    saving synced dict to {}'.format(one_syn_dict['path']))\n",
    "            with open(one_syn_dict['path'], 'wb') as fp:\n",
    "                pickle.dump(one_syn_dict, fp)\n",
    "            \n",
    "        one_syn_dict['t_p'] = np.load(t_p_path, mmap_mode='r')\n",
    "    print('Done with sync_all')\n",
    "    return\n",
    "   \n",
    "\n",
    "def bout_dict_from_pd_mismatched_streams(bout_pd: pd.DataFrame, all_syn_dict: dict, s_f_key: str='wav') -> dict:\n",
    "    s_f = all_syn_dict[s_f_key]['s_f']\n",
    "\n",
    "    start_ms = bout_pd['start_ms'].values\n",
    "    len_ms = bout_pd['len_ms'].values\n",
    "    \n",
    "    bout_dict = {\n",
    "        's_f': s_f, # s_f used to get the spectrogram\n",
    "        's_f_nidq': all_syn_dict['nidq']['s_f'],\n",
    "        's_f_ap_0': all_syn_dict['ap_0']['s_f'],\n",
    "        'start_ms': start_ms,\n",
    "        'len_ms': len_ms,\n",
    "        'start_sample_naive': ( start_ms * s_f * 0.001).astype(np.int64),\n",
    "        'start_sample_nidq': np.array([np.where(all_syn_dict['nidq']['t_0'] > start)[0][0] for start in start_ms*0.001]),\n",
    "        'start_sample_wav': np.array([np.where(all_syn_dict['wav']['t_0'] > start)[0][0] for start in start_ms*0.001])\n",
    "    }\n",
    "    \n",
    "    # Edit to remove bout starts > length of ap_0 recording\n",
    "    keep = bout_dict['start_sample_wav'] <= len(all_syn_dict['wav']['t_p'])\n",
    "    bout_dict['start_ms'] = bout_dict['start_ms'][keep]\n",
    "    bout_dict['len_ms'] = bout_dict['len_ms'][keep]\n",
    "    bout_dict['start_sample_naive'] = bout_dict['start_sample_naive'][keep]\n",
    "    bout_dict['start_sample_nidq'] = bout_dict['start_sample_nidq'][keep]\n",
    "    bout_dict['start_sample_wav'] = bout_dict['start_sample_wav'][keep]\n",
    "    \n",
    "    start_ms_ap_0 = all_syn_dict['wav']['t_p'][bout_dict['start_sample_wav']]*1000\n",
    "    \n",
    "    bout_dict['start_ms_ap_0'] = start_ms_ap_0\n",
    "    bout_dict['start_sample_ap_0'] = np.array([np.where(all_syn_dict['ap_0']['t_0'] > start)[0][0] for start in start_ms_ap_0*0.001])\n",
    "    bout_dict['start_sample_ap_0'] = (bout_dict['start_sample_ap_0']).astype(np.int64)\n",
    "    bout_dict['end_sample_ap_0'] = bout_dict['start_sample_ap_0'] + (bout_dict['len_ms'] * bout_dict['s_f_ap_0'] * 0.001).astype(np.int64)\n",
    "    \n",
    "    ## update the bout pandas dataframe with the synced columns\n",
    "    bout_pd = bout_pd.head(keep.sum()) # trim bout_pd to bouts within ap_0 recording\n",
    "    for k in ['start_ms_ap_0', 'start_sample_ap_0', 'len_ms', 'start_ms', 'start_sample_naive']:\n",
    "        warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "        bout_pd[k] = bout_dict[k]\n",
    "        warnings.resetwarnings()\n",
    "\n",
    "    return bout_dict, bout_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record epochs with mismatched streams and which stream is shortest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_streams = {\n",
    "    '0700_g0': (False,),\n",
    "    '1000_g0': (True, 'ap_0'),\n",
    "    '2355_g0': (False,),\n",
    "    '2705_g0': (False,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_r5r13_24 2024-08-08 1000_g0 syncing..\n",
      " sync nidq...\n",
      "  t_prime file /mnt/cube/chronic_ephys/der/z_r5r13_24/2024-08-08/sglx/1000_g0/nidq-tp.npy not found or forced computation, getting the events\n",
      "(2, 13971)\n",
      "(2, 7240)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 209570288 is out of bounds for axis 0 with size 209557048",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# run sync\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mismatched_streams[this_epoch][\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43msync_all_mismatched_streams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_syn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43msess_par\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mref_stream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmismatched_streams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mthis_epoch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     sy\u001b[38;5;241m.\u001b[39msync_all(all_syn_dict,sess_par[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref_stream\u001b[39m\u001b[38;5;124m'\u001b[39m],force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[22], line 38\u001b[0m, in \u001b[0;36msync_all_mismatched_streams\u001b[0;34m(all_syn_dict, ref_stream, short_stream, force)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(one_syn_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevt_arr\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:first_len_evt_arr]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(one_syn_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevt_arr\u001b[39m\u001b[38;5;124m'\u001b[39m][:,one_len_evt_arr\u001b[38;5;241m-\u001b[39mlast_len_evt_arr:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 38\u001b[0m t_prime \u001b[38;5;241m=\u001b[39m \u001b[43mscu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevt_arr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mfirst_len_evt_arr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mone_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevt_arr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mone_len_evt_arr\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlast_len_evt_arr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mone_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mone_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mone_restart\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevt_arr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mfirst_len_evt_arr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mref_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevt_arr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mref_len_evt_arr\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlast_len_evt_arr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mref_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mref_syn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref_restart\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m    saving t_prime array to \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m t_p_path)\n\u001b[1;32m     48\u001b[0m np\u001b[38;5;241m.\u001b[39msave(t_p_path, t_prime)\n",
      "File \u001b[0;32m/mnt/cube/lo/envs/ceciestunepipe/ceciestunepipe/util/syncutil.py:61\u001b[0m, in \u001b[0;36msync_to_pattern\u001b[0;34m(x_ttl, t, x_0_ttl, t_0)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of edges in the syn ttl events of pattern and target dont match\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# if all checks out, do the deed\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m t_0_edge \u001b[38;5;241m=\u001b[39m \u001b[43mt_0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_0_ttl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     62\u001b[0m sample_edge \u001b[38;5;241m=\u001b[39m x_ttl[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# the interpolation function. fill_value='extrapolate' allows extrapolation from zero and until the last time stamp\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# careful, this could lead to negative time, but it is the correct way to do it.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# interpolation function interpolates time as a target, t_0=f(sample) with true values at the edges\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 209570288 is out of bounds for axis 0 with size 209557048"
     ]
    }
   ],
   "source": [
    "# loop through epochs:\n",
    "# epoch_list = ['0700_g0', '1000_g0', '2355_g0','2705_g0']\n",
    "epoch_list = ['1000_g0']\n",
    "\n",
    "for this_epoch in epoch_list:\n",
    "    \n",
    "    sess_par['epoch'] = this_epoch\n",
    "    epoch_struct = et.sgl_struct(sess_par,sess_par['epoch'],ephys_software=sess_par['ephys_software'])\n",
    "\n",
    "    ##### synchronization - sglx #####\n",
    "    print(sess_par['bird'],sess_par['sess'],this_epoch,'syncing..')\n",
    "    if sess_par['ephys_software'] == 'sglx':\n",
    "        # get epoch files\n",
    "        sgl_folders, sgl_files = sglu.sgl_file_struct(epoch_struct['folders']['sglx'])\n",
    "        run_meta_files = {k:v[0] for k,v in sgl_files.items()}\n",
    "        run_recordings = {k:sglex.SpikeGLXRecordingExtractor(sglu.get_data_meta_path(v)[0]) for k,v in run_meta_files.items()}\n",
    "\n",
    "        # get streams, from raw recording extractors and preprocessed data\n",
    "        all_streams = list(run_recordings.keys()) + ['wav'] ### might want to just remove this\n",
    "        # get sync pattern\n",
    "        all_syn_dict = {k:sy.get_syn_pattern(run_recordings,epoch_struct,k,force=False) for k in all_streams}\n",
    "        # run sync\n",
    "        if mismatched_streams[this_epoch][0]:\n",
    "            sync_all_mismatched_streams(all_syn_dict,sess_par['ref_stream'],mismatched_streams[this_epoch][1],force=False)\n",
    "        else:\n",
    "            sy.sync_all(all_syn_dict,sess_par['ref_stream'],force=False)\n",
    "\n",
    "        # load bouts\n",
    "        hparams, bout_pd = sb.load_bouts(sess_par['bird'],sess_par['sess'],'', derived_folder='bouts_sglx',bout_file_key='bout_auto_file')\n",
    "        # keep only epoch bouts\n",
    "        logger.info('bouts from this epoch {}'.format(sess_par['epoch']))\n",
    "        drop_condition = ~bout_pd['file'].str.contains(sess_par['epoch'])\n",
    "        bout_pd.drop(bout_pd[drop_condition].index, inplace=True)\n",
    "        bout_pd.reset_index(drop=True, inplace=True)\n",
    "        # sync bouts to spike time base\n",
    "        bout_dict, bout_syn_pd = sy.bout_dict_from_pd(bout_pd,all_syn_dict,s_f_key='wav')\n",
    "        # if mismatched_streams[this_epoch][0]:\n",
    "        #     bout_dict, bout_syn_pd = bout_dict_from_pd_mismatched_streams(bout_pd,all_syn_dict,s_f_key='wav')\n",
    "        # else:\n",
    "        #     bout_dict, bout_syn_pd = sy.bout_dict_from_pd(bout_pd,all_syn_dict,s_f_key='wav')\n",
    "        # store epoch synced bout info\n",
    "        bout_syn_pd['bird'] = sess_par['bird']\n",
    "        bout_syn_pd['sess'] = sess_par['sess']\n",
    "        bout_syn_pd['epoch'] = sess_par['epoch']\n",
    "        bout_syn_pd_all.append(bout_syn_pd)\n",
    "        # save synced bouts\n",
    "        bout_dict_path = os.path.join(epoch_struct['folders']['derived'],'bout_dict_ap0.pkl')\n",
    "        with open(bout_dict_path, 'wb') as handle:\n",
    "            pickle.dump(bout_dict, handle)\n",
    "        bout_pd_path = os.path.join(epoch_struct['folders']['derived'],'bout_pd_ap0.pkl')\n",
    "        bout_pd.to_pickle(bout_pd_path)\n",
    "        logger.info('saved syncronized bout dict and pandas dataframe to {}, {}'.format(bout_dict_path, bout_pd_path))\n",
    "\n",
    "        if len(sess_par['stim_sess']) > 0:\n",
    "            # syn_ttl comes from the digital pin, syn_sine_ttl from the sine\n",
    "            event_name = 'wav_stim'\n",
    "            ttl_ev_name = event_name + '_sync_sine_ttl' \n",
    "            # get the events npy file\n",
    "            npy_stim_path = os.path.join(epoch_struct['folders']['derived'],ttl_ev_name + '_evt.npy')\n",
    "            stream_stim_path = os.path.join(epoch_struct['folders']['derived'],event_name + '.npy')\n",
    "            trial_ttl = np.load(npy_stim_path)\n",
    "            # epoch may not have trials - if so ttl file will be empty\n",
    "            if len(trial_ttl) > 0:\n",
    "                trial_stream = np.load(stream_stim_path,mmap_mode='r')\n",
    "                # get sampling frequency\n",
    "                stim_s_f = int(all_syn_dict['nidq']['s_f'])\n",
    "                # load the stimulus name - frequency tag dictionary\n",
    "                stim_tags_dict = preproc_sglx.load_stim_tags_dict(sess_par['stim_sess'],sess_par['bird'])\n",
    "                # get trial tagged dataframe\n",
    "                trial_tagged_pd = su.get_trials_pd(trial_ttl, trial_stream, stim_s_f,on_signal=sess_par['on_signal'],\n",
    "                                                   tag_chan=sess_par['trial_tag_chan'],stim_tags_dict=stim_tags_dict,\n",
    "                                                   trial_is_onof=True)\n",
    "                # sync stim\n",
    "                trial_dict, trial_syn_pd = sy.trial_syn_from_pd(trial_tagged_pd,all_syn_dict,s_f_key='nidq')\n",
    "                # store epoch synced stim info\n",
    "                trial_syn_pd['bird'] = sess_par['bird']\n",
    "                trial_syn_pd['sess'] = sess_par['sess']\n",
    "                trial_syn_pd['epoch'] = this_epoch\n",
    "                trial_syn_pd_all.append(trial_syn_pd)\n",
    "                # save synced stim\n",
    "                stim_dict_path = os.path.join(epoch_struct['folders']['derived'],'stim_dict_ap0.pkl')\n",
    "                stim_pd_path = os.path.join(epoch_struct['folders']['derived'],'stim_pd_ap0.pkl')\n",
    "                with open(stim_dict_path,'wb') as handle:\n",
    "                    pickle.dump(trial_dict,handle)\n",
    "                trial_syn_pd.to_pickle(stim_pd_path)\n",
    "                logger.info('saved syncronized stim dict and pandas dataframe to {}, {}'.format(stim_dict_path, stim_pd_path))\n",
    "\n",
    "    ###### synchronization - oe #####\n",
    "    elif sess_par['ephys_software'] == 'oe':\n",
    "        # get epoch files\n",
    "        run_recordings = {'oeb':preproc_oe.get_oe_cont_recording(exp_struct,this_epoch)}\n",
    "\n",
    "        # make an all_syn_dict\n",
    "        mic_file_name = os.path.join(exp_struct['folders']['derived'],this_epoch,'wav_mic-npy_meta.pickle')\n",
    "        with open(mic_file_name, 'rb') as handle:\n",
    "            wav_mic_meta = pickle.load(handle)\n",
    "        all_syn_dict = {'wav': {'s_f':wav_mic_meta['s_f']}, \n",
    "                       'ap_0': {'s_f':run_recordings['oeb'].get_sampling_frequency()},\n",
    "                       'nidq': {'s_f':run_recordings['oeb'].get_sampling_frequency()}}\n",
    "        # make bouts pandas file for this session - match sglx format, streams already synced\n",
    "        bout_oe_struct = et.get_exp_struct(sess_par['bird'],sess_par['sess'],sort=sess_par['sort'],ephys_software='bouts_oe')\n",
    "        bout_pd_path = os.path.join(bout_oe_struct['folders']['derived'], 'bout_auto.pickle')\n",
    "        bout_syn_pd = pd.read_pickle(bout_pd_path)\n",
    "        bout_dict = preproc_oe.bout_dict_from_pd(bout_syn_pd,all_syn_dict)\n",
    "        # store epoch synced bout info\n",
    "        bout_syn_pd['bird'] = sess_par['bird']\n",
    "        bout_syn_pd['sess'] = sess_par['sess']\n",
    "        bout_syn_pd['epoch'] = this_epoch\n",
    "        bout_syn_pd_all.append(bout_syn_pd)\n",
    "        # save synced bouts\n",
    "        bout_dict_path = os.path.join(epoch_struct['folders']['derived'],'bout_dict_oe.pkl')\n",
    "        bout_pd_path = os.path.join(epoch_struct['folders']['derived'],'bout_pd_oe.pkl')\n",
    "        with open(bout_dict_path,'wb') as handle:\n",
    "            pickle.dump(bout_dict,handle)\n",
    "        bout_syn_pd.to_pickle(bout_pd_path)\n",
    "\n",
    "        if len(sess_par['stim_sess']) > 0:\n",
    "            # this epoch name - get recording events path\n",
    "            raw_folder = exp_struct['folders']['oe']\n",
    "            epoch_path = os.path.join(raw_folder,this_epoch)\n",
    "            node_path = preproc_oe.get_default_node(exp_struct,this_epoch)\n",
    "            rec_path = preproc_oe.get_default_recording(node_path)\n",
    "            events_path = os.path.join(rec_path,'events/Network_Events-102.0/TEXT_group_1/')\n",
    "            # load stim lables / onsets\n",
    "            stim_labels = np.load(os.path.join(events_path,'text.npy'))\n",
    "            stim_onsets = np.load(os.path.join(events_path,'timestamps.npy'))\n",
    "\n",
    "            # get stim onsets and offsets\n",
    "            stim_on_all = []; stim_off_all = []; \n",
    "            stim_proc_path_all = []; stim_exp_path_all = [];\n",
    "            stim_map_dir_all = []; stim_id_all = [];\n",
    "            # loop through stim\n",
    "            for stim_i in range(len(stim_labels)):\n",
    "                this_stim_label = stim_labels[stim_i].astype('str')\n",
    "                this_stim_onset = stim_onsets[stim_i]\n",
    "                if this_stim_label[:4] == 'stim':\n",
    "                    stim_exp_file = this_stim_label[5:]\n",
    "                    # get stim preprocessing directory\n",
    "                    stim_file_split = stim_exp_file.split('/')\n",
    "                    stim_map_i = np.where([stim_file_split[i] in list(stim_map_dict.keys()) for i in range(len(stim_file_split))])[0][0]\n",
    "                    stim_map_dir = stim_map_dict[stim_file_split[stim_map_i]]\n",
    "                    # get remaining stim file path - identical for experiment and preprocessing\n",
    "                    remaining_stim_file = '/'.join(stim_file_split[stim_map_i+1:])\n",
    "                    # processing file location\n",
    "                    stim_file = os.path.join(stim_map_dir,remaining_stim_file)\n",
    "                    # load stim and get length\n",
    "                    sf,this_wav = wavfile.read(stim_file,mmap=True)\n",
    "                    stim_len = this_wav.shape[0]/sf\n",
    "                    # get length of stim in samples - round up\n",
    "                    stim_samp_len = int(np.ceil(stim_len * bout_dict['s_f']))\n",
    "                    # get stim on / off\n",
    "                    stim_on_all.append(this_stim_onset)\n",
    "                    stim_off_all.append(this_stim_onset+stim_samp_len)  \n",
    "                    stim_proc_path_all.append(stim_file)\n",
    "                    stim_exp_path_all.append(stim_exp_file)\n",
    "                    stim_map_dir_all.append(stim_map_dir)\n",
    "                    stim_id_all.append(remaining_stim_file)\n",
    "\n",
    "            # make into a pd - oe already synced\n",
    "            stim_on_all_np = np.array(stim_on_all).astype('int')\n",
    "            stim_off_all_np = np.array(stim_off_all).astype('int')\n",
    "            stim_on_all_np_ms = 1000*(stim_on_all_np/bout_dict['s_f'])\n",
    "            stim_off_all_np_ms = 1000*(stim_off_all_np/bout_dict['s_f'])\n",
    "            trial_syn_pd = pd.DataFrame(np.vstack([stim_on_all_np,\n",
    "                                                stim_off_all_np,\n",
    "                                                stim_on_all_np_ms,\n",
    "                                                stim_off_all_np_ms,\n",
    "                                                stim_off_all_np_ms-stim_on_all_np_ms,\n",
    "                                                stim_proc_path_all,\n",
    "                                                stim_exp_path_all,\n",
    "                                                stim_map_dir_all,\n",
    "                                                stim_id_all]).T,\n",
    "            columns=['start_sample','end_sample','start_ms','end_ms','len_ms',\n",
    "                     'proc_file','exp_file','map_dir','stim_id'])\n",
    "            trial_syn_pd['start_sample'] = trial_syn_pd['start_sample'].astype('int')\n",
    "            trial_syn_pd['end_sample'] = trial_syn_pd['end_sample'].astype('int')\n",
    "            trial_syn_pd['start_ms'] = trial_syn_pd['start_ms'].astype('float')\n",
    "            trial_syn_pd['len_ms'] = trial_syn_pd['len_ms'].astype('float')\n",
    "            # store epoch synced stim info\n",
    "            trial_syn_pd['bird'] = sess_par['bird']\n",
    "            trial_syn_pd['sess'] = sess_par['sess']\n",
    "            trial_syn_pd['epoch'] = this_epoch\n",
    "            trial_syn_pd_all.append(trial_syn_pd)\n",
    "            trial_dict = {\n",
    "                's_f': all_syn_dict['wav']['s_f'],\n",
    "                'ap_0':all_syn_dict['ap_0']['s_f'],\n",
    "                'nidq':all_syn_dict['nidq']['s_f'],\n",
    "                'start_ms':trial_syn_pd['start_ms'],\n",
    "                'len_ms':trial_syn_pd['len_ms'],\n",
    "                'start_sample':trial_syn_pd['start_sample'],\n",
    "                'end_sample':trial_syn_pd['end_sample'],\n",
    "                'proc_file':trial_syn_pd['proc_file'],\n",
    "                'exp_file':trial_syn_pd['exp_file'],\n",
    "                'map_dir':trial_syn_pd['map_dir'],\n",
    "                'stim_id':trial_syn_pd['stim_id']}\n",
    "            # save synced stim\n",
    "            stim_dict_path = os.path.join(epoch_struct['folders']['derived'],'stim_dict_ap0.pkl')\n",
    "            stim_pd_path = os.path.join(epoch_struct['folders']['derived'],'stim_pd_ap0.pkl')\n",
    "            with open(stim_dict_path,'wb') as handle:\n",
    "                pickle.dump(trial_dict,handle)\n",
    "            trial_syn_pd.to_pickle(stim_pd_path)\n",
    "            print('saved syncronized stim dict and pandas dataframe to {}, {}'.format(stim_dict_path, stim_pd_path))\n",
    "            \n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After handling all errors, save outputs and log preprocessing complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate list of synced bout data frames from each epoch and save\n",
    "bout_syn_pd_all_cat = pd.concat(bout_syn_pd_all)\n",
    "sb.save_auto_bouts(bout_syn_pd_all_cat,sess_par,hparams,software=sess_par['ephys_software'],bout_file_key='bout_sync_file')\n",
    "\n",
    "# stim sess save the all sync epoch stim data frame as well\n",
    "if len(sess_par['stim_sess']) > 0:\n",
    "    trial_syn_pd_all_cat = pd.concat(trial_syn_pd_all)\n",
    "    sb.save_auto_bouts(trial_syn_pd_all_cat,sess_par,hparams,software=sess_par['ephys_software'],bout_file_key='stim_sync_file')\n",
    "\n",
    "# # log preprocessing complete without error\n",
    "# log_dir = os.path.join('/mnt/cube/chronic_ephys/log', sess_par['bird'], sess_par['sess'])\n",
    "# with open(os.path.join(log_dir,'preprocessing.log'), 'w') as f:\n",
    "#     f.write(sess_par['bird']+' '+sess_par['sess']+' preprocessing complete without error\\nEpochs '+', '.join(sess_epochs)+' processed\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "songproc",
   "language": "python",
   "name": "songproc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
